\documentclass[11pt,a4paper,spanish]{book}
\usepackage{estilo_unir-1}
\usepackage{apacite}

\usepackage{hyperref}
\numberwithin{equation}{chapter}
%\usepackage{caption}
\numberwithin{figure}{chapter}
%\usepackage{chngcntr}
%\counterwithin{equation}{chapter}
%\counterwithin{figure}{chapter}
%\counterwithin{table}{chapter}
%\renewcommand\theequation{\thechapter.\arabic{equation}}
%\renewcommand\thefigure{\thechapter.\arabic{figure}}
%\renewcommand\thetable{\thechapter.\arabic{table}}
%\renewcommand\theequation{\thechapter.\arabic{equation}}
%\counterwithin{figure}{chapter}
%\counterwithin{table}{chapter}
%\renewcommand\thefigure{\thechapter.\arabic{figure}}
%\renewcommand\thetable{\thechapter.\arabic{table}}
%\renewcommand\thefigure{\thechapter.\arabic{figure}}
%\makeatletter
%\renewcommand\p@figure{\thechapter..\arabic{figure}}
%\makeatother

\usepackage{listings}
\lstset{basicstyle=\ttfamily}


%---------------------------
%título del trabajo y autor
%---------------------------
\title{Reconocimiento de patrones en sistemas industriales para la detección de fallos mediante algoritmos de Aprendizaje Automático}
\titulacion{Máster Universitario en Inteligencia Artificial}
\author{Jorge Elicer Lambraño Arroyo}
\date{11 de Septiembre de 2025}
\director{Gabriel Mauricio Ramírez Villegas}
\nombreciudad{Bogotá (Colombia)}


%---------------------------
%marges
%---------------------------
%\usepackage[margin=1.9cm]{geometry}
%---------------------------
%---------------------------
%---------------------------
%---------------------------
\begin{document}
\renewcommand{\listfigurename}{Índice de Ilustraciones}
\renewcommand{\listtablename}{Índice de Tablas}
\renewcommand{\contentsname}{Índice de Contenidos}
\renewcommand{\figurename}{Figura}
\renewcommand{\tablename}{Tabla} 

\maketitle

\frontmatter
\tableofcontents
\listoffigures
\listoftables

\chapter{Resumen}
% {\bf Nota:} En este apartado se introducirá un breve resumen en español del trabajo 
% realizado (extensión máxima: 150 palabras). Este resumen debe incluir el objetivo o 
% propósito de la investigación, la metodología, los resultados y las conclusiones.
El presente proyecto aborda la temática de  la detección de anomalías en rodamientos 
utilizando el CWRU Bearing Dataset como fuente de información. El objetivo principal 
consiste en comparar el desempeño de diversos modelos de aprendizaje automático con el 
fin de identificar cuál resulta más apropiado para el desarrollo de un detector de 
anomalías robusto, eficiente y aplicable en entornos industriales. Para ello, se 
implementaron diferentes etapas de preprocesamiento,  caracterización de las señales 
temporales y estrategías durante la fase el entrenamiento de los modelos, con el 
propósito de obtener un modelo optimizado con altas métricas de rendimiento que facilite
la tarea de detección de anomalías.Los resultados alcanzaron métricas superiores al 
90\% de exactitud, precisión, sensibilidad, F1-score y área bajo la curva RoC, lo que 
demuestra la viabilidad de aplicar machine learning en la detección temprana de fallas
en rodamientos

{\bf Palabras Clave:} Aprendizaje automático; CWRU Bearing Dataset; Detección de anomalías;
Mantenimiento predictivo; Series de tiempo.

\chapter{Abstract}
% {\bf Nota:} En este apartado se introducirá un breve resumen en español del trabajo
% realizado (extensión máxima: 150 palabras). Este resumen debe incluir el objetivo o 
% propósito de la investigación, la metodología, los resultados y las conclusiones.
The present project addresses the topic of anomaly detection in bearings using the CWRU
Bearing Dataset as the source of information. The main objective is to compare the 
performance of various machine learning models in order to identify which one is the most
suitable for the development of a robust, efficient, and industrially applicable anomaly
detector. To this end, different preprocessing stages, time-series signal characterization,
and training strategies were implemented with the purpose of obtaining an optimized model
with high performance metrics that facilitates the anomaly detection task. The results 
achieved over 90\% in accuracy, precision, recall, F1-score and RoC curve, demonstrating
the feasibility of applying machine learning for early fault detection in bearings.

{\bf Palabras Clave:} Machine Learning; CWRU Bearing Dataset; Anomaly detectino;
Predictive maintenance; Time series.




\mainmatter
\chapter{Introducción}

% El primer capítulo es siempre una introducción. En la introducción se debe resumir 
% de forma esquemática % pero suficientemente clara lo esencial de cada una de las partes 
% del trabajo. La lectura de este primer capítulo ha de dar una idea clara de lo que se 
% pretendía, las conclusiones a las que se ha llegado y del procedimiento seguido.

% Como tal, es uno de los capítulos más importantes de la memoria. Las ideas principales
% a transmitir son la identificación del problema a tratar, la justificación de su 
% importancia, los objetivos generales a grandes rasgos y un adelanto de la contribución
% que esperas hacer.

% Típicamente una introducción tiene tres apartados:
% \begin{itemize}
% \item Motivación / justificación del tema a tratar
% \item Planteamiento del trabajo
% \item Estructura del trabajo
% \end{itemize}

En el presente trabajo se propone el desarrollo de un sistema de detección de anomalías
basado en técnicas de aprendizaje automático, aplicado al análisis de datos operativos
del sector industrial. A lo largo de este documento, el lector encontrará una 
contextualización para abordar el problema de la detección de anomalías, un estudio del
estado del arte en técnicas de detección de anomalías, un análisis detallado de los 
datos industriales utilizados, el diseño e implementación de diferentes modelos de 
aprendizaje automático, así como una comparación rigurosa de su rendimiento frente a 
soluciones existentes.


En el sector industrial, las anomalías se consideran como aquellos datos atípicos por 
fuera de los rangos operativos de los equipos, y en muchos casos están relacionadas con 
fallas, necesidad de mantenimiento o deficiencias en el funcionamiento. Usualmente, en 
el contexto industrial, los fallos están relacionados con valores atípicos. Antes que 
ocurra un fallo, suelen presentarse valores anómalos.  


La detección de estas anomalías, es una herramienta para la prevención de fallos. Una 
implementación de detección temprana de anomalías, puede prevenir fallos, reducir el 
riesgo de daños a la infraestructura y costos adicionales por reparaciones de equipos.
Como resultado, se obtienen como beneficios un aumento en la vida útil de los equipos 
gracias al mantenimiento preventivo, mejores condiciones de seguridad, optimización de
recursos y se reduce el riesgo de que los procesos productivos se detengan por fallos.  


Debido a la amplia gama de beneficios que ofrece la detección de anomalías para la 
prevención de fallos, ésta se ha convertido en un foco de estudio e investigación en 
muchos campos de la ingeniería, la estadística, la ciencia de datos y la informática.
En el presente trabajo se expone cómo se ha abordado este tema y se propone el 
desarrollo de una solución para detección de anomalías en operaciones industriales.
El enfoque de la solución está orientado a modelos estadísticos y de aprendizaje
automático. 


\section{Planteamiento del problema}

La detección de anomalías se ha convertido en un proceso clave, puesto que permite
identificar datos atípicos que podrían resultar en fallos o riesgos importantes en el
funcionamiento de muchas industrias. Las anomalías, aunque poco frecuentes, pueden 
representar eventos críticos que comprometen la integridad de procesos, generan 
pérdidas económicas, o incluso ponen en riesgo vidas humanas. 


Un alto porcentaje de las fallas que se presentan en maquinaria industrial pueden ser
detectadas gracias a anomalías que ocurren antes de éstas. Como resultado, una detección
temprana de anomalías permite la identificación de futuros fallos y realizar algún tipo 
de mantenimiento preventivo o acción correctiva con el fin de evitarlo. Como 
consecuencia, se obtiene un aumento de la vida útil de los equipos y disminución en el
número y costo de reparaciones. 


La detección de anomalías no se limita solamente al sector industrial. Existen casos de
uso donde se aplican conceptos de visión por computadora y deep learning para la 
detección de anomalías en imágenes médicas,\cite{zhou2021proxy}, \cite{guo2024encoder}, 
\cite{lu2024heterogeneous} \& \cite{zhong2022video}.
Del mismo modo, existen aplicaciones en el campo de la videovigilancia que combinan la 
visión por computadora y la detección de anomalías. \cite{zhong2022video}, 
\cite{zeng2021graph} \& \cite{zhang2022influence}. 
Estas soluciones identifican comportamientos inusuales en imágenes y videos que podrían
pasar desapercibidos ante la observación humana.


En los sectores financieros y de ciberseguridad, la detección de  anomalías es aplicada
para facilitar la tarea de detección de fraudes. Estos sistemas permiten identificar
transacciones inusuales o patrones de comportamiento sospechosos en tiempo real, lo que
resulta crucial para prevenir pérdidas económicas y violaciones de seguridad. Para este
caso de uso, los datos de entrada del sistema de detección son diferentes al anterior,
es decir, en lugar de requerir videos o imágenes como datos de entrada, se requiere
información de las transacciones. Por este gran espectro de aplicaciones la detección de
anomalías se ha convertido en una foco de estudio de la informática. Actualmente, se 
investigan modelos cada vez más precisos y escalables que puedan adaptarse a entornos 
dinámicos y con grandes volúmenes de datos. 


La gran diversidad de formatos que pueden tener los datos de entrada supone un reto para
la detección de anomalías. Cada uno de éstos requiere técnicas específicas de 
procesamiento, que pueden ser más o menos complejos dependiendo del tipo de formato. 
Algunos ejemplos de formatos de datos a los que se han diseñado detectores de anomalías 
son: tendencias y mediciones con respecto al tiempo, para el caso de variables 
industriales (presión, temperatura, humedad, flujo); señales de audio en el rango de 
frecuencia audible o fuera del rango audible humano; imágenes y videos en el caso de 
videoanalítica, y también textos, archivos y otros formatos no estructurados. 


A pesar de esa gran variedad, existen muchos algoritmos que permiten extraer 
características de los diferentes formatos de entrada. Esta etapa de extracción de 
características transforma los datos crudos en vectores o representaciones que 
pueden ser analizados de manera uniforme. Una vez obtenidas estas representaciones, 
es posible aplicar estrategias comunes de detección de anomalías, como técnicas 
estadísticas, modelos de aprendizaje automático supervisado o no supervisado, e 
incluso enfoques basados en aprendizaje profundo. Esto permite unificar el tratamiento 
de datos heterogéneos.


Además de su amplia variedad, los datos industriales suelen presentar características 
aumentan la complejidad de su procesamiento, como por ejemplo, ser multivariantes, tener
una relación señal a ruido desfavorable, contener un alto porcentaje de valores 
faltantes o nulos, estar organizados en series temporales, entre otras. Esto plantea un 
desafío técnico para un sistema de monitoreo tradicional. En este contexto, surge la 
siguiente necesidad: ¿Cómo se puede construir un sistema de detección de anomalías 
altamente efectivo, preciso y flexible para poder adaptarse a situaciones complejas y 
que permita identificar de manera confiable comportamientos atípicos en entornos 
industriales reales?. 


El presente trabajo se plantea como una respuesta a esta necesidad, mediante el 
desarrollo de una solución de detección de anomalías, orientada al análisis de variables
industriales. La solución buscará alcanzar un desempeño comparable o superior al de las
técnicas existentes en el estado del arte. 


\section{Motivación}

Este proyecto surge de la necesidad de contar con soluciones inteligentes capaces de 
prevenir eventos críticos mediante la detección de anomalías, el análisis de datos y el
aprendizaje automático. 
La  importancia de la detección temprana de anomalías como solución ante este problema 
radica en que permite realizar acciones correctivas que eviten que se generen fallas en 
los sistemas industriales, puesto que los fallos normalmente van precedidos de 
comportamientos anómalos en las variables que se están censando.


Tomar acciones correctivas antes que se presenten fallos tienen múltiples beneficios,
puesto que los fallos pueden ocasionar que procesos industriales se detengan generando
incumplimientos en la producción. Los efectos de los fallos son todavía más graves para
aquellos procesos que funcionan continuamente, las interrupciones se convierten en 
grandes pérdidas económicas e incumplimiento de las metas de producción. 
De esta manera, la prevención de fallos supone grandes ahorros a las industrias y reduce
el riesgo de que se generen interrupciones. De esta manera, por medio de la detección
temprana, se mejora la confiabilidad operativa y se contribuye a la reducción de costos
asociados con paradas no planificadas, daños a equipos y pérdidas en la producción.


La detección de anomalías puede integrarse con otras tecnologías como los agentes 
inteligentes. 
La integración con éstos tiene un mayor impacto en la eficiencia de los sistemas 
industriales. Esta integración permite que se acepte o se rechace los resultados de 
los detectores de anomalías y que se tomen las diferentes decisiones 
\cite{jidiga2014anomaly}.


La necesidad de soluciones cada vez más automatizadas, con constante monitoreo y que 
generen una gran cantidad de datos, hace que la detección de anomalías sea esencial 
para garantizar la seguridad, confiabilidad y eficiencia de sistemas industriales y en 
otras áreas. 
La capacidad de identificar estos eventos de manera oportuna no solo permite prevenir 
fallos o fraudes, sino que también habilita nuevos modelos de mantenimiento inteligente 
y optimización de recursos y beneficios a largo plazo. Investigar y perfeccionar 
técnicas de detección de anomalías representa un reto de gran relevancia para el 
desarrollo de múltiples sectores industriales.


\section{Plantamiento del trabajo}

Tomando como punto de partida la necesidad de identificar anomalías en entornos 
industriales con el fin de prevenir fallos y optimizar procesos, este trabajo propone 
el desarrollo de una solución basada en aprendizaje automático que permita detectar 
comportamientos atípicos a partir del análisis de variables operativas. El objetivo es 
modelar el comportamiento normal del sistema y reconocer, de manera oportuna y precisa, 
desviaciones significativas que puedan indicar fallas incipientes. 


Para ello, se implementarán estrategias basadas en aprendizaje automático, que permitan 
modelar el comportamiento normal del sistema y reconocer desviaciones significativas de 
manera oportuna y precisa. 
El desarrollo de esta solución requiere múltiples fases. 
El primer paso es el análisis exploratorio de los datos de las variables de entrada con 
el fin de entender la estructura,  calidad y distribución de las variables que 
alimentarán el modelo. 
En esta etapa se realiza una evaluación de la calidad de los datos, donde se mide el 
porcentaje de datos faltantes, duplicados y valores atípicos. 
Adicionalmente, se calculan los estadísticos más importantes y las correlaciones entre 
las variables. 
También se definen las transformaciones de las variables y todas las operaciones que 
hacen parte del preprocesamiento de los datos.


Basándose en los formatos de los datos de entrada y salida se construirá un modelo 
de aprendizaje de máquina. 
El modelo debe ser capaz de procesar los datos en el formato de entrada 
(por ejemplo, series temporales, datos multivariantes) y generar una salida que permita
identificar si los valores de entrada corresponden a una anomalía o no. 
La solución será evaluada bajo criterios de precisión, sensibilidad, especificidad y 
eficiencia computacional.


Se espera que el modelo resultante de esta fase genere resultados con una calidad igual 
o superior a los modelos que hoy en día se encuentran en estado del  arte. Con el fin de
hacer una comparación entre los modelos del estado del arte y el modelo propuesto, se 
miden métricas de rendimiento bajo condiciones experimentales homogéneas. 
Asimismo, se discutirán las ventajas, limitaciones y oportunidades de mejora de la 
solución propuesta, considerando su potencial implementación en entornos industriales
reales.


\section{Estructura de la memoria}


El presente trabajo tiene la siguiente estructura:


\textbf{Capítulo 1:} Se brinda una breve introducción del presente trabajo. 
Se expone la motivación que ha impulsado este desarrollo y por qué es importante 
profundizar en la detección de anomalías. 
Además, se plantea con claridad el problema de investigación, formulado en términos 
precisos. 
Finalmente, se describe la estructura del presente documento, explicando de forma clara 
cómo está dividido el contenido del estudio y qué se espera encontrar en cada capítulo.


\textbf{Capítulo 2:} Se realiza una revisión exhaustiva del estado del arte, también se 
realiza un barrido de las diferentes técnicas que están relacionadas con la detección 
de anomalías. 
En ella se examinan los enfoques más relevantes utilizados en la literatura, abarcando 
desde una perspectiva tradicional así como enfoques más modernos basados en inteligencia 
artificial y aprendizaje automático. 
Asimismo, se analizan los resultados que han obtenido otros investigadores en este campo, 
junto con las estrategias y métodos implementados.


\textbf{Capítulo 3:} Se presentan los objetivos generales y específicos que guían cada 
etapa de desarrollo del presente estudio. 
El objetivo general enuncia de manera precisa la finalidad del estudio y su impacto 
esperado. 
Por otro lado, los objetivos específicos desglosan dicho propósito en metas alcanzables 
y medibles que estructuran el camino metodológico a seguir. 
Esta sección es fundamental, ya que permite al lector entender qué se espera lograr y 
cómo se evaluará el cumplimiento de las metas.


\textbf{Capítulo 4:} Se describe detalladamente la metodología adoptada para llevar a 
cabo la investigación. 
Se explican los pasos y procedimientos que se seguirán, desde la recolección de los 
datos y su procesamiento, hasta la implementación de los modelos y técnicas 
seleccionadas para la detección de anomalías. 
Esta sección incluye el conjunto de herramientas y tecnologías utilizadas, así como los 
criterios de validación y evaluación de los modelos a implementar. 
La metodología constituye el puente entre los objetivos planteados y los resultados que
se desean obtener.


\textbf{Capítulo 5:} Se presentan los resultados obtenidos tras la aplicación de la 
metodología previamente descrita. 
Se analizan de forma crítica y detallada los datos generados, utilizando 
representaciones visuales y métricas pertinentes que faciliten su comprensión. 
Además, se comparan los resultados con estudios previos y se realizan las respectivas 
interpretaciones, lo que permite identificar patrones, tendencias y posibles 
explicaciones para los hallazgos obtenidos. 


\textbf{Capítulo 6:} Se presentan conclusiones derivadas del estudio, las cuales 
sintetizan los principales hallazgos y resultados obtenidos y su relevancia en el 
contexto del problema investigado. 
Se reflexiona sobre el cumplimiento de los objetivos y la metodología planteada. 
Además, se identifican las oportunidades de mejora, reconociendo aquellos aspectos 
que podrían explorarse con mayor profundidad, y dan espacio a diversas líneas de 
investigación futura que permitan extender y enriquecer el presente trabajo.


\chapter{Contexto y Estado del Arte}

\section{Contexto}

Para el desarrollo de este proyecto se ha seleccionado el CWRU Bearing Dataset, proporcionado por el Case Western Reserve University Bearing Data Center. A nivel académico e industrial, este conjunto de datos es una de las principales referencias para el análisis del rendimiento de motores eléctricos y la detección temprana de fallas en rodamientos. Se ha convertido en uno de los recursos más confiables y estandarizados para el entrenamiento y comparación de modelos de detección de fallas basados en aprendizaje automático.


Con la información contenida en este dataset se pueden construir modelos para clasificar la existencia o no de una falla, clasificar el tipo de falla y clasificar el nivel de gravedad de la falla. Por ser un recurso ideal para el estudio de datos en el análisis de vibraciones, ingeniería mecánica, diagnóstico de fallas y machine learning, ha sido seleccionado para el desarrollo de este proyecto. \cite{caseWesternBearingData}. 


Este conjunto de datos se obtiene a partir de un banco de pruebas que incluye un motor de 2 HP, sensores de par, un dinamómetro y sistemas de control electrónico. El dataset contiene las señales de salida resultantes al realizar mediciones de vibración en los rodamientos del motor, las vibraciones se midieron a través de acelerómetros.  


Los rodamientos del motor fueron sometidos a defectos inducidos de diferentes tamaños y ubicaciones (bola, pista interna y pista externa), con el fin de generar condiciones reales de fallo. Las mediciones fueron registradas mediante acelerómetros ubicados en tres posiciones clave (extremo de transmisión, extremo del ventilador y base), con una frecuencia de muestreo de 48kHz y 12kHz. 


Se decidió trabajar con una versión corregida y simplificada del dataset CWRU desarrollada por \cite{rigas2024marine}. El CWRU Bearing Dataset original se encontraba en formato .mat, con algunos metadatos inconsistentes que no seguían un estándar uniforme y redundancias en algunos de sus archivos. El dataset se encuentra disponible en Github. 


Para mejorar su usabilidad en proyectos de machine learning, los datos fueron convertidos a formato .npz, conservando únicamente las series temporales necesarias para el análisis. De esta manera, la nueva versión presenta un conjunto de datos más limpio, consistente y listo para ser cargado directamente en entornos Python, lo que facilita su aplicación en algoritmos de diagnóstico y detección de fallas.


Los archivos del dataset siguen una convención de nombres que permite identificar fácilmente las condiciones de operación y el tipo de falla asociada a cada serie temporal. Para los datos con fallas, el esquema utilizado es:


\begin{verbatim}
RPM_Fault_Diameter_End
\end{verbatim}


Donde, 

\begin{itemize}
	\item \texttt{RPM}: Revoluciones por minuto del motor. (\texttt{1797}, \texttt{1772}, \texttt{1750} o \texttt{1730}). La reducción de la velocidad se debe a que se le agrega carga al motor.
	\item \texttt{Fault}: Tipo de defecto introducido en el rodamiento (\texttt{IR} para pista interna, \texttt{B} para elemento rodante, \texttt{OR@6}, \texttt{OR@3} o \texttt{OR@12} para pista externa en distintas posiciones).
	\item \texttt{Diameter}: Diámetro del defecto en milésimas de pulgada (\texttt{7}, \texttt{14}, \texttt{21} o \texttt{28}).
	\item \texttt{End}: Ubicación del sensor y frecuencia de muestreo (\texttt{FE}, \texttt{DE12}, \texttt{DE48}).
\end{itemize}


En el caso de las condiciones normales (sin fallas), los archivos se nombran simplemente como \texttt{RPM\_Normal}. 
Cada archivo \texttt{.npz} puede contener una o varias series temporales, organizadas en claves llamadas \texttt{DE}, \texttt{FE} o \texttt{BA}, que corresponden a los datos capturados en el drive end, fan end o base del motor, respectivamente \cite{rigas2024marine}.


\section{Estado del arte}

\subsection{Fundamentos de detección de anomalías}

Con el fin de manejar una misma definición de anomalías, se toma como base la 
definición de anomalías propuesta por \cite{leon2012anomalias}, quien define las 
anomalías como “patrones en los datos medidos en un proceso que no se ajustan a un 
concepto bien definido de comportamiento normal”. 
En el contexto industrial, las anomalías se manifiestan como desviaciones en las 
mediciones de variables como presión, temperatura, flujo, vibración o consumo 
energético, y suelen indicar la presencia de fenómenos anómalos como desgastes, 
obstrucciones, fallos mecánicos, errores de configuración o condiciones inusuales 
de operación.


Adicionalmente, \cite{leon2012anomalias} una falla como una “desviación no 
permitida, con respecto a lo aceptable, usual o condición nominal, de a los 
menos una propiedad característica o parámetro de un sistema”. De esta manera 
existe una clara diferencia entre una anomalía y una falla: mientras una anomalía 
es una señal temprana de un comportamiento inusual o anormal en los datos, una 
falla es la consecuencia de una anomalía desatendida, donde el sistema ya ha 
salido de su estado funcional o seguro. 


A diferencia de simples fluctuaciones tolerables del sistema, las anomalías 
representan rupturas en la regularidad del proceso, que pueden ser precursoras de 
fallos o eventos no deseados. En muchos casos, estas irregularidades no son evidentes 
a simple vista, ya que pueden ser sutiles, contextuales (dependen del momento o 
condiciones del proceso), o estar enmascaradas por ruido o variabilidad natural.
El objetivo de un detector de anomalías es garantizar la operabilidad de procesos 
industriales al reconocer de forma anticipada una anomalía, permitiendo realizar 
acciones correctivas. 


La identificación oportuna de anomalías es la base de una estrategia eficaz de 
mantenimiento predictivo, cabe resaltar que una anomalía no implica necesariamente
la existencia de una falla, pero sí constituye un indicio de un comportamiento inusual.
Esta detección temprana permite permite anticiparse a fallos graves y optimización de 
los tiempos operativos.

\subsection{Enfoques tradicionales de detección de anomalías}

Las técnicas de detección de anomalías que se han desarrollado son variadas. 
La literatura ha propuesto diversos enfoques para la detección de anomalías. 
Primero se encuentran los métodos tradicionales basados en reglas de negocio 
determinísticas, donde las variables medidas deben complir condiciones determinadas 
previamente, y en caso de no cumplirlas, se determina que se ha presentado una anomalía 
en el proceso. 


Luego están técnicas estadísticas univariadas qué incluye el análisis de la 
distribución de los datos, incluyendo el análisis de la desviación estándar y otros 
estadísticos. 
Asimismo existen técnicas más avanzadas basadas en aprendizaje automático y modelos de 
aprendizaje profundo. 
Cada uno de estos enfoques presenta ventajas y limitaciones que deben considerarse según 
el contexto de aplicación, el tipo de datos disponibles, la criticidad del sistema, y los 
recursos computacionales con los que se cuenta.


El enfoque más tradicional para enfrentar el problema de detección de fallas consiste en 
el diseño de algoritmos basados en reglas \cite{gami2024datacleansing}. Los algoritmos 
basados en reglas consisten en una serie de condiciones lógicas que deben cumplirse para
catalogar un comportamiento como una anomalía. 
Las reglas suelen construirse a partir del 
conocimiento experto y se basan en umbrales fijos sobre variables monitoreadas, como 
temperatura, presión o vibración. Por ejemplo, si la temperatura de un motor supera un 
valor crítico durante un cierto periodo, se genera una alerta. 


Aunque este enfoque puede ser efectivo en sistemas muy simples y bien caracterizados, 
presenta importantes limitaciones en entornos industriales modernos. 
Entre sus principales desventajas está su rigidez frente a variaciones normales del 
sistema, su escasa capacidad de adaptación a condiciones cambiantes y la dificultad 
de mantener reglas actualizadas en sistemas complejos o multivariantes. 
Como consecuencia de lo anterior, el desempeño de los modelos de detección de 
anomalías basados en este enfoque suele degradarse con el tiempo. 
Otra gran desventaja de los sistemas basados en reglas es su dependencia del 
conocimiento experto para definir los umbrales y condiciones, esto limita su 
escalabilidad, flexibilidad y capacidad de generalización ante nuevos contextos 
operativos.

\subsubsection{Enfoques de detección de anomalías univariados.}

En entornos donde los procesos son relativamente estables o las variables monitoreadas 
pueden analizarse de forma individual, es común aplicar técnicas de detección de 
anomalías univariadas. 
Estas técnicas se centran en evaluar una sola variable a lo largo del tiempo, y 
permiten identificar desviaciones significativas del comportamiento normal mediante el 
uso principalmente de herramientas estadísticas. Ejemplos de estás técnicas de 
detección son las siguientes: 

\begin{itemize}
	\item Shewhart Chart.
	\item Suma Acumulada.
	\item Media Móvil con Ponderación Exponencial.
\end{itemize}

Shewhart Chart, también conocido como gráfico de control, según \cite{chen1998shewhart}, es una herramienta basada en estadística para monitorear la variabilidad a lo largo del tiempo. Consiste en comparar los datos de una serie univariada con límites de control, lo cual permite identificar cambios significativos en la variable u otras anomalías. Los valores de los límites de control superior e inferior se calculan basándose en la media y la desviación estándar. El intervalo entre los límites de control permite identificar si un rango de datos se encuentra dentro un rango aceptable. 

\cite{lopezcano2023spc}, propone una metodología para construir un gráfico de control, la cual  se tienen en cuenta dos fases fundamentales: primero la determinación de los límites de control a partir de un proceso estadístico, y la segunda fase sería la determinación de la estrategia de muestreo, donde estos valores se monitorizan bajo el criterio de los límites fijados, observando así comportamientos y patrones que permitan sacar hipótesis. Lo que se logra con esto es controlar procesos mediante gráficos para comprobar visualmente que el proceso esté bajo control.

Además del diseño de gráficos de control,\cite{lopezcano2023spc}, propone una serie de patrones que pueden indicar que el proceso deja de estar bajo control: Muestras fuera de los intervalos de crecimiento, una serie de muestras seguidas creciendo o decreciendo, muestras alternadas por encima y debajo de la línea central LC, y una serie de muestras debajo o encima de la línea central. 

Suma Acumulada, CumSum. \cite{contreras2024cusum}. CumSum ha sido una de las primeras estrategias para detectar cambios en series de tiempo, más específicamente para la detección de cambios en la media. De esta manera, la señal de entrada se divide en segmentos con diferentes valores medios. En la Figura~\ref{fig:figPuntosCambiosSegmentos} se aprecia los puntos de cambios calulados para una señal. 


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{media/puntos-cambios-segmenteos.png}
    \caption{Puntos de cambio detectados y división de la señal segmentos  \protect\cite{contreras2024cusum} }
    \label{fig:figPuntosCambiosSegmentos}
\end{figure}


La Media Móvil con Ponderación Exponencial (Exponentially Weighted Moving Average, EWMA) \cite{fan1996ewma} se utiliza en la detección de anomalías al suavizar datos y resaltar cambios significativos. La EWMA da más peso a las observaciones recientes, permite identificar desviaciones de la norma, facilitando la detección de patrones inusuales en series temporales. Esta técnica es aplicada en trading de divisas. 


\subsection{Métodos basados en aprendizaje de máquina}


Los sistemas industriales suelen ser complejos puesto que integran muchos componentes, y esto hace que la detección de anomalías sea una tarea bastante compleja porque técnicas univariadas para detección de anomalías no suelen ser suficientes. Muchas de las técnicas tradicionales de detección de anomalías, como las anteriormente mencionadas, fallan cuando los datos alcanzan una alta volumetría y también cuando se generan a muy alta velocidad y son multidimensionales \cite{thudumu2020survey}.


Para afrontar el problema de múltiples variables y sistemas más complejos, se implementa como alternativa los algoritmos de aprendizaje de máquina. Las ventajas de los algoritmos de aprendizaje de máquina sobre las estrategias basadas en reglas y estadísticas son múltiples. En primer lugar, mientras que los modelos de aprendizaje de máquina construyen las reglas que describen los patrones de los datos, en el modelamiento estadístico hay que formalizar las relaciones entre las variables a través de ecuaciones matemáticas, \cite{ij2018statvsml}. 


Otra ventaja de los algoritmos de aprendizaje de máquina sobre el modelado estadístico tradicional es que la capacidad de los modelos para aprender los patrones a partir de los datos proporcionados sin la necesidad de asumir una distribución específica de los datos. En el modelado estadístico normalmente se asume que los datos siguen alguna distribución, y es a partir de allí que se construyen las ecuaciones y reglas con que se modelan los datos. 


Existe una gran variedad de modelos de aprendizaje de máquina, una gran diversidad de estos modelos han sido implementados en el problema de detección de anomalías. Se puede encontrar en la literatura el uso de modelos de aprendizaje supervisado para la detección de anomalías. 


En el trabajo de \cite{baradaran2025predictivemaintenanceelectricmotors}, se  evaluaron varios modelos como Naive Bayes, SVM, Random Forest, k-NN, métodos de boosting, entre otros, para poder detectar alguno de los tres estados posibles de un motor: Saludable (Healthy), averiado (Broken), y necesita mantenimiento (Needs Preventive Maintenance). De este estrato se concluye que tiene sentido la aplicación de un modelo de clasificación de aprendizaje de máquina en ese caso de uso. Sin embargo, es importante la elección del modelo, puesto que se observan diferencias en las precisiones de los clasificadores.   


\subsubsection{Naive Bayes}

Naive Bayes es un algoritmo de aprendizaje automático que se basa en fundamentos estadísticos, principalmente en el teorema de Bayes el cual permite actualizar la probabilidad de una hipótesis a partir de nueva evidencia. Este teorema relaciona probabilidades de dos eventos A y B a través de la dependencia condicional, asumiendo independencia condicional, es decir, que los valores de una característica son independientes de la presencia o propiedades de las demás características. Según \cite{han2012datamining}, la Equación~\ref{eq:eq1NaiveBayes} describe al teorema de Bayes.


\begin{equation}\label{eq:eq1NaiveBayes}
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}
\end{equation}


Donde $P(A \mid B)$ es la Probabilidad posterior de que ocurra el evento $A$, dado que ya ocurrió el evento $B$; $P(B \mid A)$ es la Probabilidad condicional de que ocurra B si sabemos que el evento $A$ ya ocurrió; $P(A)$ es Probabilidad a priori de que ocurra $A$, sin considerar ninguna información adicional y $P(B)$ es Probabilidad total de que ocurra $B$, considerando todos los casos posibles.


Los clasificadores basados en el algoritmo Naive Bayes utilizan las probabilidades a priori para estimar la probabilidad de ocurrencia del evento objetivo. Aplican de manera secuencial la fórmula del Teorema de Bayes, asumiendo independencia condicional entre las características con el fin de simplificar los cálculos del modelo. Como resultado del proceso anterior, se obtiene la Equación~\ref{eq:eq1NaiveBayesProd} matemática para calcular la probabilidad estimada de que el evento objetivo ocurra. \cite{salman2024rf}. 


\begin{equation}\label{eq:eq1NaiveBayesProd}
P(C|x_1, x_2, ..., x_n) = P(C) \prod_{i = 0}^{n} P(x_i \mid C)
\end{equation}


A pesar de su simplicidad, el modelo de clasificación basado en Naive Bayes ofrece un rendimiento sorprendentemente eficaz en tareas de clasificación, y en muchos casos supera en precisión a métodos más complejos, especialmente cuando se trabaja con grandes volúmenes de datos y características independientes. Su eficiencia computacional lo convierte en una opción ideal para aplicaciones en tiempo real. Además, requiere una menor cantidad de datos para entrenamiento en comparación con otros algoritmos.


Otras investigaciones han encontrado qué el modelo de clasificación Naive-Bayes da buenos resultados de precisión. \cite{chen2016xgboost} entrenó dos modelos de clasificación para dos datasets diferentes, con Naive-Bayes. Los resultados fueron qué los modelos logran una precisión aproximada del 99.9\% y 99.65\%, respectivamente. Por otro lado, en múltiples trabajos, se encuentra qué el algoritmo qué mejor desempeño tiene a la hora de detectar anomalías es el random forest. Como por ejemplo el trabajo de \cite{sharma2022predictive} y \cite{yu2025tkeo}.


\subsubsection{Máquinas de soporte vectorial}

Las Máquinas de Vectores Soporte, SVM en por sus siglas en inglés: Support Vector Machine, tienen su principio en la aplicación de trabajos que requieren teoría de aprendizaje estadístico, son usadas para resolver tareas de clasificación y regresión. Según \cite{amat2017maquinas}, el principio fundamental en una SVM es encontrar un hiperplano óptimo encargado de la clasificación de los datos, el cual maximiza el margen entre las clases, es decir, la distancia entre los puntos de datos más cercanos de cada clase (conocidos como vectores soporte) y el hiperplano. En la Figura~\ref{figSVM} se observa cómo una Máquina de Vectores de Soporte clasifica datos en un hiperplano. 


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{media/svm-dm.png}
    \caption{Clasificación de datos en un espacio bidimensional. En (a) se observan los datos separados a través de un hiperplano óptimo, mientras que en (b) se muestra los posibles hiperplanos que se pueden aplicar para la separación de los datos.  \protect\cite{suarez2014svm} }
    \label{fig:figSVM}
\end{figure}


Mientras que la mayoría de métodos de aprendizaje se centran en minimizar los errores empíricos, las SVMs radican la minimización del riesgo estructural, es decir, se busca minimizar los errores empíricos, pero al mismo tiempo, maximizar la capacidad del modelo para generalizar. Este enfoque no solo mejora la capacidad de generalización del modelo ante nuevos datos y lo hacen robusto frente al sobreajuste. Otra ventaja que tienen los modelos SVM es el uso de funciones núcleo (kernel functions) que transforman el espacio permitiendo manejar problemas no linealmente separables. 


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{media/svm-kun.png}
    \caption{Detección de anomalías en datos de vuelo utilizando One-Class SVM.  \protect\cite{qin2022flight} }
    \label{fig:figSVMflight}
\end{figure}


Originalmente este sistema fue creado para problemas de clasificación binaria, pero puede ser aplicado en problemas de clasificación multiclase, y también en problemas de detección de anomalías. En la Figura~\ref{fig:figSVMflight}, propuesta por \cite{qin2022flight}, se presenta un estudio para identificar anomalías en datos reales de vuelo, usando como base el principio One-Class SVM. Se construyó un sistema para detectar eventos anómalos en tiempo real, en la aplicación de este estudio se logró que es un método muy viable para este tipo de escenarios obteniendo una alta capacidad de detección de anomalías y mantuvo una baja tasa de falsas alarmas lo cual es crítico para monitoreo de aeronaves.


\subsubsection{k-NN}


K-Nearest-Neighbours (kNN) es quizás uno de los métodos más populares y sencillos para implementación de algoritmos de extracción y análisis de datos en una variedad de temas de investigación en informática. Los algoritmos KNN se han empleado de manera extensa en el ámbito de la investigación y se les reconoce como uno de los diez algoritmos más destacados en minería de datos \cite{witten2005data}. En este tiempo del Big Data, las metodologías KNN ofrecen una forma especialmente eficaz para reconocer patrones valiosos y crear algoritmos de razonamiento fundamentados en casos para la inteligencia artificial (IA). \cite{wu2008top}


El estudio de la selección de vecinos más cercanos ha sido ampliamente investigada, observada y analizada con el fin de determinar la mediana de proximidad, se puede decir que lo que se desea obtener con todo esto es centralizar la construcción de funciones de distancia para medir la proximidad. Es esencial contar con una función a distancia que permita elegir los puntos de vecinos más cercanos, la cual pueda operar de manera efectiva con la gran parte de las muestras de entrenamiento. \cite{zhang2022influence}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{media/knn-zhou.png}
    \caption{Detección de fallos usando FD‑kNN y PC‑kNN.  \protect\cite{zhou2015faultdetection} }
    \label{fig:figKnnZhou}
\end{figure}


En el campo de la detección de fallos, se han aplicado métodos basados en KNN : FD (Feature-Dependent) y PC (Prototype Constrained), cada uno sirvió para el estudio del comportamiento en situaciones controladas, logrando identificar altas tasas de detección de fallos y bajas tasas de falsas alarmas, proveyendo una solución simple y efectiva para detección de fallos en sistemas complejos, ofreciendo así resultados sólidos en aplicaciones industriales reales, validando su aplicabilidad práctica.\cite{zhou2015faultdetection}.


\subsubsection{Random Forest}


Los modelos basados en Random Forest son una de las herramientas más populares a la hora de construir modelos de predicción para clasificación, regresión y minería de datos \cite{rushall2013rf}. Consisten en un conjunto de árboles de decisiones entrenados con diferentes subconjuntos de datos aplicando estrategias como bagging y boosting. Los árboles de decisión dividen el hiperespacio en segmentos según los datos de prueba. Random Forest utiliza un conjunto de árboles de decisión para mejorar la capacidad de generalización.  


Según, \cite{salman2024rf}, una de las principales ventajas de los modelos basados en Random Forest es su versatilidad porque puede ser aplicado en regresión y clasificación,  además, tiene una buena tolerancia ante valores atípicos y el ruido, gracias a ésto han sido uno de los métodos de investigación más populares en el área de minería de datos. Adicionalmente, durante el entrenamiento de los árboles de decisión, las características más relevantes son ubicadas en la raíz del árbol de decisión, esto facilita la interpretación y búsqueda de patrones en los datos. 


Los modelos basados en algoritmos Random Forest, según \cite{canovas2017random}, tienen como una de sus principales ventajas que son simples de entrenar y al mismo tiempo pueden alcanzar un rendimiento similar al de otras técnicas más complejas, y también, son robustos ante un gran porcentaje de datos nulos. Por esta razón, esta técnica es ampliamente utilizada en la detección de fraudes bancarios y otros tipos de anomalías en los datos. 


\cite{yu2025tkeo} compararon el rendimiento de los modelos de clasificación SVM  y Random Forest, aplicando el operador TKEO sobre el dataset CWRU como técnica de preprocesamiento para predecir alguno de 17 categorías relacionadas con la salud del motor. Ambos modelos tuvieron buenos resultados y demostraron qué bajo ciertas condiciones de cargas es viable utilizar el TKEO debido a su eficiencia con dataset grandes.


\cite{sharma2022predictive} realiza la evaluación de cinco modelos de clasificación usando datasets diferentes para predecir el momento para mantenimiento. El objetivo es determinar cuál de los modelos funciona mejor. En este estudio curiosamente se vuelve a resaltar a Random Forest como el modelo con la mayor precisión.


\subsubsection{XGBoost}

El algoritmo XG Boost (Extreme Gradient Boosting) es una técnica de aprendizaje supervisado que se basa en árboles de decisión \cite{chen2016xgboost}.  Es una poderosa herramienta de aprendizaje automático que fusiona eficacia, adaptabilidad y excelente desempeño para solucionar problemas complicados de clasificación y regresión. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{media/xgboost-salman.png}
    \caption{Visualización del algoritmo XGBoost. Ensamble de árboles de decisión.  \protect\cite{salman2024rf} }
    \label{fig:figXGBoostSalman}
\end{figure}


Se basa principalmente en la implementación del algoritmo de boosting de árboles de decisión. XGBoost consiste en un ensamblaje secuencial de árboles de decisión. Los árboles se agregan secuencialmente a fin de aprender del resultado de los árboles previos y corregir el error producido por los mismos, hasta que ya no se pueda corregir más dicho error \cite{salman2024rf}.  En XGBoost, a esta técnica donde cada árbol recién construido busca rectificar los fallos realizados por los árboles previos con el objetivo de generar un modelo más robusto y exacto se le denomina “gradiente descendiente”. \cite{espinoza2020rf_xgboost}.

\subsubsection{CatBoost}

CatBoost es otro algoritmo de Machine Learning basado en árboles de decisión, su principal ventaja es su eficiente capacidad de predicción para características de tipo categórico  \cite{ibrahim2020catboost}. A diferencia de otros modelos basados en Boosting, esta técnica no requiere aplicar técnicas de hot-encoding manualmente, puesto que es capaz de manejar las variables categóricas de forma nativa. CatBoost implementa gradient boosting, el cual hace uso de árboles de decisión binarios como base para sus predictores \cite{prokhorenkova2018catboost}.


CatBoost se ha implementado en la detección de robo de electricidad. Fue seleccionado este modelo por su eficiencia para manejar un gran porcentaje de características de datos de tipo categórico durante el entrenamiento del modelo como en su predicción, \cite{hussain2021catboost}. Asimismo, otra ventaja de este algoritmo es su interpretabilidad, en contraste con otras soluciones basadas en boosting. 


\subsubsection{LightGBM}


Este es uno de los algoritmos más populares de aprendizaje automático, es ampliamente implementado en soluciones que manejen grandes volúmenes de datos y de gran dimensionalidad. LightGBM usa estrategias claves, GOSS (Gradient-based One-Side Sampling) que consiste en usar las observaciones con mas gradientes grandes ya que son las instancias donde el modelo pierde exactitud en la predicción, y por otro lado EFB (Exclusive Feature Bundling) que agrupa las variables que usualmente toman valores nulos o iguales a cero. Estas variables son empaquetadas lo que reduce considerablemente la cantidad de variables a considerar.


Se estima que LightGBM logra entrenar mucho más rápido (hasta 20 veces), que otros modelos basados en árboles de decisión, lo que permite el procesamiento de grandes volúmenes de datos y un buen rendimiento en contextos donde la velocidad es crítica, como en sistemas en tiempo real, \cite{ke2017lightgbm}. En IoT, por ejemplo, se ha implementado LightGBM para la detección de anomalías en sistemas de enfriamiento. \cite{yanabe2020lightgbm}, y también se ha implementado este método para la detección de anomalías en el tráfico de una red. \cite{islam2020lightgbm}. 


\cite{delasmorenas2025bearing} probaron tres modelos de clasificación (XGBoost, CatBoost y LightGBM). Los 3 demostraron buenos resultados en precisión (98\% aprox.). Este estudio termina con la conclusión que para ese caso de uso el algoritmo de LightGBM obtiene los mejores resultados, y que se debe considerar como opción en casos de Edge Computing.


\subsection{Métodos basados en aprendizaje no supervisado}

En los anteriores trabajos se encuentra qué los dataset con los qué trabajaron eran dataset previamente etiquetados. Sin embargo esto puede ser una limitación en aplicaciones industriales o de negocio en las que el trabajo de etiquetado no se ha hecho.


Existen casos de detección de anomalías donde se busca que el modelo segmenta un conjunto de datos no etiquetados previamente de manera que sea éste el que defina qué es una anomalía y que no. Esta técnica de segmentación en la que los conjuntos de datos no han sido previamente etiquetados se denomina aprendizaje no supervisado. Aplicando este enfoque,  \cite{Cao_2025} escribe un artículo hablando de distintas técnicas para la detección de anomalías a través de mecanismos de aislamiento. Se mencionan varias técnicas, pero de las más destacadas se encuentra la Insolation Forest.


\subsubsection{Isolation Forest}

El algoritmo de Isolation Forest (IF) es usado y conocido popularmente para detección de anomalías, produce heat maps los cuales son mapas de puntuación de anomalías que presentan artefactos visuales, al ser un modelo robusto este permite que las puntuaciones de anomalía sean más estables, especialmente a lo largo de regiones con puntuaciones constantes dando como resultado una mejora en la precisión de puntuaciones y detección de anomalías, sin sacrificar eficiencia computacional. \cite{liu2021isoletionforest}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{media/liu-isolation-forest.png}
    \caption{Puntos de datos y mapas de puntuación de anomalías de dos grupos de puntos normalmente distribuidos.  \protect\cite{liu2021isoletionforest} }
    \label{fig:figIsolationForest}
\end{figure}


La Figura~\ref{figIsolationForest} revela que, aunque hay dos grupos normales claramente definidos, el Isolation Forest estándar introduce sesgos geométricos (como triángulos y grupos falsos) en el mapa de anomalía. Esto es lo que impulsa la propuesta del Extended Isolation Forest (EIF), que utiliza hiperplanos con pendientes aleatorias. \cite{liu2021isoletionforest}.


\subsection{Enfoques basados en Deep Learning}

En la búsqueda de este estado del arte se ha procurado buscar los modelos qué utilizan modelos clásicos por su interpretabilidad y fácil entrenamiento. Sin embargo, es posible que estos modelos no sean capaces de ofrecer buenos resultados y se deba escalar hacia el uso de algoritmos basados en aprendizaje profundo. El aprendizaje profundo utiliza redes neuronales para el modelamiento de los datos, con la ventaja que estos modelos son capaces de aprender patrones de forma jerárquica. \cite{raj2024bearing}, utilizó los datos del dataset CWRU y redes neuronales convolucionales en el entrenamiento, alcanzando precisiones mayores al 98\%.


\subsubsection{Redes Neuronales}

Las redes neuronales son modelos de aprendizaje automático inspirados en el cerebro humano. Estos modelos están formados por unidades simples llamadas neuronas artificiales conectadas entre sí, las cuales reciben señales de entrada, las procesa y pasa el resultado por una función que genera una salida. Este tipo de modelo suele variar su arquitectura dependiendo su aplicación, por ejemplo, redes de una capa (perceptrón simple), útil para problemas lineales y redes multicapa (MLP), capaces de resolver problemas no lineales mucho más complejos.


Las redes neuronales se han convertido en uno de los modelos más populares en el ámbito del aprendizaje automático, gracias a su capacidad para alcanzar altos niveles de exactitud y obtener resultados sobresalientes en diversas métricas de desempeño. Se han aplicado para problemas de clasificación, predicción en campos como la visión artificial, el procesamiento digital de señales y la detección de fallos. 


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{media/neuona-artificial.png}
    \caption{Puntos de datos y mapas de puntuación de anomalías de dos grupos de puntos normalmente distribuidos.  \protect\cite{fernandez2013redes} }
    \label{fig:figNauronaArtificial}
\end{figure}


En la Figura~\ref{fig:figNauronaArtificial} se ilustra una comparación entre una neurona biológica y una neurona artificial, en la cual resaltamos tres elementos funcionales: el primero serían las conexiones (sinapsis) con pesos que seria el peso que tiene cada conexión entre las neuronas, segundo el sumador que recibe las señales y calcula la suma total y por ultimo funcion de activacion que limita la amplitud de la señal de salida de la neurona. \cite{fernandez2013redes}.


Según \cite{rivera2007redes}, la gran ventaja de las redes neuronales frente a otros modelos es su aprendizaje adaptativo, puesto que éstas son capaces de realizar una tarea (como regresión o clasificación) a partir de los datos de entrada, los cuales terminan siendo representados como entradas y pesos.  Además de su capacidad de auto-organizar la información, es decir, crea su propia representación de la información en el proceso de entrenamiento. 


Adicionalmente, según \cite{varela2011redes}, una de las principales ventajas de las redes neuronales es la amplia diversidad de arquitecturas que pueden adoptarse, lo que les otorga gran flexibilidad para abordar distintos tipos de problemas. Sin embargo, uno de los principales retos en su diseño consiste en seleccionar la arquitectura más adecuada y acompañarla de un algoritmo de entrenamiento eficiente, de modo que el modelo logre identificar y discriminar patrones relevantes en los datos. 


A pesar de sus grandes ventajas, las redes neuronales existen algunas consideraciones a la hora de implementar algoritmos de redes neuronales. Primero, las redes neuronales requieren un entrenamiento intensivo en tiempo y recursos, se necesita un conjunto de datos grande para evitar sobreajuste (overfitting) y, finalmente, el funcionamiento interno es difícil de interpretar. Pese a todo esto, las redes neuronales tienen mucha relevancia como un modelo fundamental y eficaz. \cite{larranaga2021redes}


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{media/sistema-neuronal.png}
    \caption{Sistema global de proceso de una red neuronal.  \protect\cite{larranaga2021redes} }
    \label{fig:figSistemaNeuronal}
\end{figure}



De forma general la jerarquía de procesamiento está definida de esa manera: Neurona, Capa, Red, Sistema neuronal. Desde la unidad más pequeña (neurona) hasta el sistema completo capaz de tomar decisiones o hacer predicciones, resaltando que la parte algorítmica es el corazón del sistema global, donde ocurre la propagación de señales y aprendizaje mediante ajuste de pesos. \cite{larranaga2021redes}


Las redes neuronales están diseñadas en una arquitectura que distribuye la información entre las neuronas y las capas, de forma que esta cuenta con tres capas principales definidas de la siguiente manera manera: capa de entrada, la cual recibe y da entrada a los datos iniciales, por otro lado tenemos capas ocultas las cuales realizan el procesamiento aplicado de operaciones lógicas y de procesamiento y por último tenemos la capa de salida que entrega el resultado final como puede ser una predicción o clasificación. \cite{larranaga2021redes} 

Aunque es cierto que existen diferentes tipos de de arquitecturas cada una se adapta según la tarea, un ejemplo de algunas de estas arquitecturas son: El perceptrón multicapa (MLP) esta es la forma más básica y simple donde todas las neuronas de una capa están conectadas con la siguiente, por otro lado tenemos las redes profundas (DNN) este modelo se caracteriza por tener muchas capas ocultas, lo que le permite ilustrarse con procesos más complejos, redes convolucionales (CNN), redes recurrentes (RNN), redes residuales (ResNet)


\cite{Zamanzadeh_Darban_2024} escribe un artículo describiendo las diferentes técnicas de Deep Learning que se pueden encontrar en la literatura para detectar anomalías en series de tiempo y clasifica los modelos en dos grupos principales: Forecasting-based donde se encuentran modelos como CNN, y reconstruction-based como por ejemplo el modelo GAN. Además brinda una lista de dataset qué han sido utilizados en otras investigaciones como Benchmark para evaluar el rendimiento de la detección de anomalías.



\subsubsection{Redes Neuronales Convolucionales CNN}


Este enfoque de redes neuronales convolucionales (CNN) enfocado en la implementación de identificar y modelar sistemas que no se comportan de manera lineal, a diferencia de otros tipos de redes neuronales como el perceptrón, las redes neuronales convolucionales toleran más el ruido de los datos y eviten problemas surgidos a partir de eso. por otro lado tenemos que esta brinda una mayor precisión, robustez y capacidad de generalización en el modelado de sistemas no lineales. \cite{LopezPacheco2017}


Este modelo ayuda a captar patrones sutiles que pueden señalar comportamientos no habituales, en muchos casos una forma común de mejorar el modelo es combinarlo con estrategias de muestreo, umbrales bien calibrados y, a veces, enfoques semisupervisados, previniendo así limitaciones en el balanceo de clases y la necesidad de validar con rigor para controlar falsos positivos. \cite{ameijeiras2021algoritmos}.


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{media/lenet-o5.png}
    \caption{Architecture of LeNet-5  \protect\cite{Zhao2024review}}
    \label{fig:figLenet}
\end{figure}


En la Figura~\ref{fig:figLenet} se observa la arquitectura de una CNN. En este caso se aplica una CNN clásica que procesa imágenes de entrada blanco y negro de 32×32 píxeles, primero se aplican filtros que detectan bordes y lineas, posteriormente la siguiente capa identifica características más complejas como formas y rasgos, despues toda esta información se convierte en un vector y pasa por capas que integrsan todo lo aprendido. \cite{Zhao2024review}





\subsection{Aplicaciones en el contexto insdustrial} 

El principal uso de la detección de anomalías en el contexto industrial es el pronóstico de fallas cuyo propósito es detectar a tiempo cuando se debe hacer mantenimiento predictivo. Esto permite anticiparse a posibles fallos en los procesos industriales, optimizando la eficacia, minimizando costos y reduciendo los tiempos de inactividad no planificados. Este mantenimiento predictivo se ve potenciado por los avances tecnológicos los cuales han demostrado mejorar la eficacia y eficiencia en muchas industrias ya que mejora la seguridad y confiabilidad de los procesos industriales y sus sistemas.


Adicionalmente, \cite{worktrek2023predictive} estima que el mercado global de mantenimiento predictivo se valoró en 7.850 millones de dólares en 2022 y se proyecta que alcance los 60.130 millones de dólares para 2030 (CAGR 29,5\%), lo que evidencia una clara viabilidad y un notable incremento no solo evidencia el valor de este tipo de soluciones sino también denota el interés creciente del mercado por incorporar estas herramientas, en consideración, estas soluciones no solo tienen solo una  justificación técnica y económica sólida, sino que también responden a una necesidad global de transición hacia sistemas industriales más resilientes, inteligentes y sostenibles.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{media/worktrek.png}
    \caption{Crecimiento del mercado de mantenimiento predictivo de 2022 a 2030  \protect\cite{worktrek2023predictive}}
    \label{fig:figWorkTrek}
\end{figure}


Este panorama estimula a la industria a buscar sistemas más eficientes, inteligentes y autónomos capaces de anticiparse a un fallo con base en patrones, recalcamos que este a su vez ha cambiado los paradigmas tradicionales de mantenimiento correctivo y preventivo, exigiendo modelos más proactivos, precisos y adaptables.


Resolver problemas no lineales, donde múltiples factores interactúan de manera compleja debido a la conexión de varios elementos, es un verdadero reto. Estas interacciones pueden dar lugar a comportamientos emergentes que son impredecibles y, a menudo, sorprendentes. En este sentido, hay pocos estudios que se centran en la detección de fallas utilizando técnicas de monitoreo y diagnóstico adaptadas específicamente a sistemas complejos como este \cite{zhang2017overview}. 


Frente a esto, la inteligencia computacional ha ganado importancia como una herramienta clave para vigilar el estado de las máquinas. Su objetivo principal es identificar fallas y anomalías a través del análisis de datos históricos y en tiempo real. Estos algoritmos ayudan a entender mejor el estado operativo de los equipos mediante un monitoreo continuo. Para que esto funcione, un sistema de monitoreo efectivo debe incluir tres fases fundamentales: la extracción de características, el diagnóstico de fallas y la predicción de su evolución. Este enfoque no solo facilita la detección de condiciones anómalas, sino que también permite localizar con precisión las fallas y estimar su posible propagación \cite{randall2011vibration}.


\cite{deloitte2022predictive} reportó que el mantenimiento predictivo aumenta el tiempo de funcionamiento del equipo entre un 10\% y un 20\%, al tiempo que reduce los costos generales de mantenimiento entre un 5\% y un 10\% y el tiempo de planificación del mantenimiento entre un 20\% y un 50\%, lo que refleja la importancia de este tipo de estudios en las industrias y se denota en factores cruciales como lo es el tiempo y los gastos, dado así que el mantenimiento debe ir mucho más allá de simplemente prevenir tiempos de inactividad de activos individuales. Predecir fallos mediante análisis avanzados puede aumentar el tiempo de actividad de los equipos hasta en un 20 \%.


Muchos papers a la hora de evaluar algoritmos de detección de fallas utilizan datasets públicos para evaluar el rendimiento de múltiples modelos. Los aquí citados han utilizado datasets de motores, que es uno de los componentes industriales de mayor interés. El dataset CWRU es uno de los dataset qué aparecen citados en varios trabajos, como el de \cite{yu2025tkeo} \& \cite{raj2024bearing} que además también trabaja con el dataset MFPT.


Hay otros trabajos qué además han propuesto una arquitectura de hardware sobre la que se desplegará el modelo de detección. Es el caso del trabajo de De las Morenas, \cite{delasmorenas2025bearing}, que proponen una infraestructura de Edge Computing donde se despliega un modelo de clasificación sobre una Raspberry Pi 3B qué está conectado a un motor, y se predicen 3 posibles estados.

La publicación de \cite{davari2021} utiliza un modelo de redes neuronales para predecir cuando las unidades de aire de los trenes necesitan mantenimiento. De esta investigación se deriva el dataset MetroPT y también el trabajo de  \cite{veloso2022metrpt}, donde describe el dataset y por qué sería una buena opción para benchmarking de modelos de detección.


Cómo paso previo antes de realizar un modelo de datos qué nos permita detectar anomalías. Es necesario analizar la naturaleza de los datos qué se van a analizar, con el fin de determinar si es pertinente hacer una tarea de limpieza o curado de los datos, así como alguna transformación de los datos. En el trabajo de \cite{yu2025tkeo}, se aplicó el operador TKEO sobre el dataset CWRU como técnica de preprocesamiento, antes de entrenar algún modelo supervisado. La decisión se debe a qué el Dataset corresponde  señales de motor, en el qué posibles anomalías se resaltan utilizando el operador.


A partir de estos avances, \cite{jardine2006review} desarrolla un enfoque de mantenimiento basado en condición, el cual se apoya en la información obtenida a través de la monitorización continua. Este modelo se compone de tres elementos esenciales: la adquisición de datos, el procesamiento de la información recolectada y la toma de decisiones orientadas al mantenimiento. La integración de estos componentes permite generar diagnósticos y pronósticos más precisos, favoreciendo el análisis de los patrones y comportamientos presentes en los datos, y optimizando la gestión del ciclo de vida de los activos \cite{jardine2006review}


\chapter{Identificación de Requisitos}


\chapter{Objetivos}

Dada la creciente necesidad de identificar comportamientos atípicos en la industria, 
se plantean una serie de objetivos claros que permiten abordar el problema desde una 
perspectiva integral. 
Estos objetivos guían el diseño, implementación y evaluación de modelos capaces de 
detectar desviaciones anomalías de manera eficiente y precisa.


\section{Objetivo General}

\textbf{Desarrollar} una solución basada en algoritmos de aprendizaje automático 
capaz de detectar anomalías en datos provenientes de entornos industriales, garantizando
un desempeño medible a través de métricas estándares.
La solución deberá alcanzar métricas de rendimiento superiores a 0.85 en métricas como
exactitud y F1, demostrando alta eficacia en la identificación de anomalías.

\section{Objetivos Específicos}

\begin{itemize}

\item Desarrollar la arquitectura de una solución para la detección de anomalías, donde 
se definan claramente los distintos componentes que la integran, sus funciones y el 
flujo de trabajo que sigue la solución, con el fin de  garantizar una implementación 
clara y coherente.


\item Analizar conjuntos de datos industriales, con el fin de garantizar que cumpla 
con los requerimientos necesarios para la implementación de algoritmos de aprendizaje 
automático y realizar los ajustes y transformaciones a los datos en caso de ser necesario. 


\item Seleccionar los modelos y algoritmos de aprendizaje automático que se integrarán
en la solución, e implementar los ajustes necesarios en sus parámetros y configuración
con el fin de optimizar su rendimiento en términos de precisión, sensibilidad y otras
métricas relevantes.


\item Evaluar el desempeño de los modelos seleccionados, utilizando un conjunto de 
condiciones experimentales homogéneas que garanticen una evaluación objetiva, 
identificando fortalezas, limitaciones y oportunidades de mejora.


\item Implementar el análisis del conjunto de datos, la selección, entrenamiento y 
evaluación de modelos, la ejecución de pruebas controladas y la disponibilización de la 
solución final, utilizando herramientas de desarrollo especializadas. 

\end{itemize}


\chapter{Metodología de trabajo}


El presente trabajo se enmarca en un enfoque cuantitativo y de tipo aplicado, ya que se 
busca diseñar, implementar y evaluar una solución basada en algoritmos de aprendizaje 
automático para la detección de anomalías en datos industriales. La investigación se 
centra en el procesamiento y análisis de datos numéricos provenientes de sistemas reales,
con el objetivo de generar métricas que sean medibles y replicables, ejemplos de estas 
métricas son precisión, sensibilidad, especificidad, F1-score y área bajo la curva ROC.
El estudio es descriptivo, en la medida en que se analizan características estadísticas 
de los datos, se aplican técnicas de preprocesamiento, y es experimental, porque se 
construyen modelos que son evaluados bajo métricas estándar de desempeño. 


\section{Metodología CRISP-DM}

Con el fin de alcanzar los objetivos propuestos del presente proyecto, se ha decidido 
basarse en los pasos de la metodología  CRISP-DM, llamada así por sus siglas en inglés 
CRoss-Industry Standard Process for Data Mining. La ventaja de aplicar las fases de esta
metodología es su enfoque a proyectos de ciencia y minería de datos e inteligencia 
artificial, y que además ofrece una guía para ajustar los procedimientos técnicos del 
proyecto a los objetivos propuestos, lo que facilita la selección de actividades y 
la ejecución del proyecto. La Figura~\ref{fig:figCrispdm} muestra cada una de las fases de la metodología 
CRISP-D.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{media/crisp-dm.png}
    \caption{Fases de la metodología {CRISP-DM}.  \protect\cite{chapman2000crisp} }
    \label{fig:figCrispdm}
\end{figure}



\subsection{Entendimiento del tema}

Aquí es donde se hace un análisis del contexto, se desarrollan los objetivos generales 
y específicos del proyecto. Esos objetivos deben girar en torno al desarrollo de una 
solución capaz de identificar comportamientos atípicos de variables industriales con un 
desempeño superior a los de métodos actuales. Acá se definen los criterios de éxito del
sistema, las métricas que se van a utilizar para evaluar los modelos, las reglas a la 
hora de realizar la comparación, y las condiciones que se espera que el modelo cumpla 
para ser implementado en ambientes industriales reales. 


\subsection{Entendimiento de los datos}


Para la etapa de entendimiento de datos se realiza una selección de un conjunto de datos
de variables industriales, los cuales pueden incluir mediciones de sensores, logs de 
máquinas, registros y tendencia de producción, entre otros. A estos datos de entrada se 
les realiza un análisis exploratorio con el propósito de identificar las variables 
relevantes, correlaciones, valores atípicos, duplicados y faltantes y calcular 
estadísticos descriptivos de las variables y su distribución.


La fase de entendimiento de los datos permite conectar los objetivos del proyecto con 
los conjuntos de datos disponibles. Aquí se define operativamente qué se considera una 
anomalía, teniendo en cuenta el contexto de los procesos industriales y las posibles 
causas de desviaciones. Es importante identificar en el conjunto de datos si se cuenta 
con una etiqueta que identifique si los datos corresponden a una anomalía o no, esto es 
importante para la fase de modelado porque esto determina si se cumplen las condiciones 
para seleccionar un algoritmo supervisado o no supervisado. 

\subsection{Preparación de los datos}


Con base en el análisis exploratorio de los datos, se seleccionan los procedimientos 
que harán parte de la fase de preprocesamiento de los datos, y también aquellos 
algoritmos que realicen transformaciones sobre los datos. Durante esta fase se eligen 
las estrategias para lidiar con datos faltantes, y las técnicas de reducción de ruido. 
En esta fase también se estudia la posibilidad de aplicar técnicas como feature engineer 
y análisis de componentes principales (PCA),  con el fin facilitar la inferencia de los 
modelos de las fases siguientes.


\subsection{Modelado}


Tal como se plantea en el segundo objetivo específico, en esta fase se implementa un 
modelo de machine learning basándose en los datos ya procesados. Se hace una selección 
rigurosa de los algoritmos a utilizar. Una vez seleccionados, se implementan y se 
comparan soluciones del estado del arte y finalmente, se hacen los respectivos ajustes 
para optimizar su rendimiento.
El conjunto de todos estos pasos hace posible la construcción de un modelo robusto que 
cumpla con los requerimientos del proyecto. 


\subsection{Evaluación}

Los modelos entrenados se evalúan mediante un conjunto homogéneo de datos y condiciones 
experimentales, siguiendo el tercer objetivo específico. Se hace una interpretación de 
los resultados de la evaluación, y se identifican con éstos, las ventajas y desventajas 
que presentan unos modelos frente a otros. 
Se hace un énfasis en el modelo propuesto en los resultados del modelo propuesto. 
Los resultados determinan en qué grado el modelo cumple con los requerimientos propuestos. 

\subsection{Despliegue}

Se propone una estrategia de despliegue donde el modelo esté disponible y pueda ser 
integrado fácilmente con otras herramientas que hacen parte del contexto industrial. 
Se abordan temas de escalabilidad, mantenimiento y actualización continua del modelo. 
Este paso garantiza el cumplimiento del objetivo general. 



\chapter{Desarrollo del trabajo}

Debido a que se han encontrado múltiples modelos de aprendizaje automático capaces de identificar si se presenta o no alguna anomalía, se plantea el desarrollo de una estrategia para realizar la comparación. El rendimiento de los modelos de aprendizaje automático en las tareas de clasificación de datos es muy bueno, se espera que las métricas al evaluar los modelos sean elevadas. 


Para poder llevar a cabo esta comparativa se ha propuesto una serie de factores a tener en cuenta, cómo será configurado el experimento, qué preprocesamiento se le hará al dataset para garantizar buenos rendimientos en los modelos, qué métricas serán seleccionadas a la hora de comparar los modelos, qué modelos harán parte de esta comparativa, y con qué criterio se selecciona al mejor modelo. Finalmente, se hace una interpretación de algunos modelos seleccionados que serán de ayuda para una mejor interpretación del conjunto de datos. 


\section{Configuración de experimentos}

 En el presente proyecto se ha decidido simplificar el problema de detección de fallas por medio de la construcción de un modelo de clasificador binario cuyo objetivo es diferenciar entre datos anómalos y datos normales provenientes de las mediciones del CWRU Bearing Dataset. Este enfoque permite establecer una primera etapa de detección de fallas, en la que el modelo actúa como un sistema de alerta temprana al identificar desviaciones respecto al comportamiento esperado de los rodamientos bajo condiciones normales de operación.
Para la construcción del clasificador binario, se organizaron los datos del CWRU Bearing Dataset en dos grandes grupos: Normales, que corresponden al funcionamiento sin fallas, y Anómalos,  que agrupan todos los registros asociados a fallas en la pista interna (\texttt{IR}), el elemento rodante (\texttt{B}) o la pista externa (\texttt{OR}), independientemente de su diámetro o ubicación.


Esta estrategia de agrupación simplifica el problema de clasificación, permitiendo contar con un modelo que funcione como un sistema de detección temprana de anomalías. En lugar de distinguir entre múltiples tipos de fallas, el clasificador se centra en identificar si los datos corresponden a un estado saludable o defectuoso del rodamiento, lo que constituye un primer paso esencial antes de avanzar hacia modelos más especializados de clasificación multiclase.


El dataset que se genere a partir del agrupamiento de los datos anómalos y no anómalos, se utilizará para entrenar todos los modelos de clasificación binaria. Adoptar esta estrategia garantiza que todos los modelos son evaluados bajo las mismas condiciones experimentales, lo que permite realizar comparaciones más justas de rendimiento. Además, usar un único dataset reduce la variabilidad introducida por particiones diferentes de los datos y se evitan sesgos en la comparación de modelos de aprendizaje.


Por el contrario, si no se emplea el mismo dataset para todos los modelos, las métricas obtenidas podrían reflejar más las diferencias en los datos de entrada que en las capacidades reales de los algoritmos. Esto claramente, dificulta la tarea de identificar qué modelo es más robusto y podría conllevar a conclusiones poco confiables. En ese sentido, mantener un dataset unificado asegura que las diferencias observadas sean atribuídas directamente a las propiedades del modelo y no a variaciones accidentales en los datos utilizados para el entrenamiento y la validación.


Para mejorar los resultados de entrenamiento, se aplicó la técnica de división del dataset en tres subconjuntos: entrenamiento, validación y evaluación (test). El conjunto de prueba se reserva exclusivamente para la comparación final de los modelos, de manera que no sean utilizados en ninguna etapa del entrenamiento. De esta manera se garantiza una evaluación objetiva y realista del rendimiento. 
Adicionalmente, el dataset de validación se emplea durante la búsqueda de hiperparámetros, permitiendo optimizar parámetros clave de los modelos con el fin de obtener la configuración con las mejores métricas de desempeño. Como resultado, se asegura que el modelo seleccionado sea aquel que genere los mejores resultados, evitando tanto el sobreajuste como conclusiones sesgadas por un mal diseño experimental.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{media/arquitecura.png}
    \caption{Arquitectura y componentes propuestos  }
    \label{fig:figArchitecture}
\end{figure}

En la Figura~\ref{fig:figArchitecture}, se observa la arquitectura diseñada para la configuración de este experimento. Esta arquitectura permite implementar un flujo de trabajo organizado, reproducible y escalable, donde cada componente cumple un rol específico dentro del ciclo de vida del desarrollo de modelos de aprendizaje automático. La arquitectura se basa en un flujo de trabajo modular, apoyado en diferentes herramientas de ciencia de datos y gestión de modelos:

\begin{itemize}

\item \textbf{Generación del dataset}. Se utilizan notebooks en \textbf{Jupyter} que hacen uso de librerías como \textbf{NumPy} y \textbf{Pandas} (basadas en Python) para la carga, limpieza, transformación y construcción del dataset final. En esta etapa se preparan los datos en el formato adecuado para los experimentos de entrenamiento.


\item \textbf{Entrenamiento de modelos}. En esta etapa, se desarrollan notebooks enfocados en el entrenamiento de modelos de machine learning mediante la librería \textbf{scikit-learn} (también basada en python). Aquí se incluyen técnicas de validación cruzada, búsqueda de hiperparámetros y evaluación de rendimiento con métricas de clasificación.


\item \textbf{Gestión de experimentos y modelos}. Para asegurar el monitoreo y reproducibilidad, se integra \textbf{MLflow}, que permite llevar un registro sistemático de los experimentos, los hiperparámetros utilizados, las métricas obtenidas y las versiones de cada modelo entrenado.


\item \textbf{Almacenamiento de metadatos}.  Toda la información relacionada con los experimentos, como configuraciones, métricas y resultados, se almacena en una base de datos \textbf{PostgreSQL}, que sirve como repositorio centralizado para la metadata del proyecto. Y allí se almacenan todos los metadatos que el servidor de MLflow genera. 


\item \textbf{Almacenamiento de modelos}. Los modelos que alcanzan mejores métricas son almacenados en \textbf{MinIO}, un sistema de almacenamiento de objetos, lo cual facilita la gestión, acceso y despliegue futuro de los modelos seleccionados.

\end{itemize}


\section{Exploración del dataset}

Como se ha mencionado anteriormente, se ha decidido trabajar con datos refinados del conjunto de datos de CWRU Bearing Dataset. El primer paso en la exploración de los datos consistió en listar los archivos disponibles en el dataset. El dataset se encuentra organizado en cuatro grandes grupos, cada uno asociado a un valor específico de revoluciones por minuto (RPM): 1797, 1772, 1750 y 1730. Dentro de cada uno de estos grupos solo existe un archivo de datos de condiciones normales, mientras que el resto de los archivos corresponden a condiciones de fallo en los rodamientos.


De esta manera, se obtiene un total de 157 archivos correspondientes a fallos frente a únicamente 4 archivos de datos normales. Este resultado es inusual en los problemas de detección de anomalías, ya que lo más frecuente es encontrar una proporción mucho menor de registros anómalos respecto a los normales. Esto permite construir un dataset balanceado para el entrenamiento de modelos, reduciendo así la necesidad de aplicar técnicas adicionales de re-muestreo para equilibrar las clases.


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{media/dataset/normal-series.png}
    \caption{Gráfica de una submuestra de las series de tiempo que corresponden a datos en condiciones normales para RPM iguales a 1772 (superior) y 1730 (inferior). }
    \label{fig:figNormalSeries}
\end{figure}


Con el fin de obtener una aproximación visual de las diferencias entre los patrones de vibración de entre los datos de condiciones normales y los datos de fallos en los rodamientos, el siguiente paso en la exploración de los datos consiste en la exploración de los datos consistió en visualizar gráficamente un segmento de las series de tiempo para ambos conjuntos de datos. La Figura~\ref{fig:figNormalSeries} y Figura~\ref{fig:figAnomalySeries}, muestran las señales en condiciones normales y en presencia de defectos respectivamente. 


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{media/dataset/anomaly-series.png}
    \caption{Gráfica de una submuestra de la serie tiempo que corresponden a datos en condiciones anómalas a 1750 RPM con el rodamiento cargado en la condición B un defecto de 0.021 pulgadas en el lado del accionamiento (superior) y a 1772 RPM con el rodamiento cargado en la condición B un defecto de 0.014 pulgadas en el lado del accionamiento (inferior). }
    \label{fig:figAnomalySeries}
\end{figure}




Cada archivo contiene mediciones de tres acelerómetros, dependiendo de las claves DE, FE o BA. Pero el número de llaves puede variar de un archivo a otros, algunos pueden contener sólo 1, mientras que otros pueden contener 3 \cite{rigas2024marine}. Al revisar detalladamente las llaves de los archivos de las series de tiempo en condiciones normales y en presencia de fallas  se obtiene el resultado de la Tabla 1. 


Según la Tabla 1, los archivos que contienen series de tiempo en condiciones normales presentan datos de los acelerómetros DE y FE, sin embargo no presentan datos de los acelerómetros de tipo BA. Esta información es de mucha importancia a la hora de construir el dataset de entrenamiento de los modelos. Por otra parte, todos los archivos de datos anómalos presentan datos de los acelerómetros de tipo DE, pero datos faltantes en los acelerómetros de tipo FE, y en mayor medida BA. 


A cada serie de tiempo se le calculan parámetros estadísticos básicos que permiten caracterizar su comportamiento y servir como insumo para la construcción de los modelos. Los parámetros a calcular son longitud de la serie, media, promedio, desviación estándar y los valores máximos y mínimos. Y se calcularon los estadísticos de estos valores, y se graficaron los histogramas de esos parámetros. La Tabla 2 muestra los resultados de los estadísticos de los parámetros calculados, donde cada fila contiene los estadísticos de los parámetros calculados. 


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{media/dataset/histogram-01.png}
    \caption{Histograma de la distribución de cada uno de los parámetros calculados para cada serie. }
    \label{fig:figHistogram01}
\end{figure}


La Figura~\ref{fig:figHistogram01} muestra los histogramas de cada uno de los parámetro de las series. Los histogramas de los parámetros máximo (b) y mínimo (a) son simétricos, lo cual es un indicio de una alta correlación. Los datos corresponden a series de tiempo es de esperarse que para estos tipos de datos la media esté concentrada en valores cercanos a cero, el histograma de distribución de la media (d) lo confirma. La longitud de las series (a) es un parámetro que no se tiene en cuenta a la hora de generar el dataset de entrenamiento porque es una característica propia de la medición. La distribución de rpm (f) muestra la cantidad de archivos por rpm medida.    


Las distribuciones del promedio (d), el máximo (b), el mínimo (c) y la desviación estándar (e) presentan un marcado sesgo hacia cero. Para atenuar este efecto y aproximarse a una distribución más similar a la normal, se aplica una transformación logarítmica. La transformación trae como resultado las gráficas de la Figura XX. En general, se puede apreciar una mejoría en las distribuciones de desviación estándar (b), máximos (a) y mínimos(d). 

Para analizar las relaciones existentes entre los distintos parámetros numéricos, se construyó la matriz de correlación entre las variables. Esta herramienta permite identificar el grado de asociación lineal entre cada par de variables, facilitando la detección de posibles dependencias o redundancias en los datos. La matriz resultante se observa en la Figura~\ref{fig:figCorrMat}. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{media/dataset/corr-mat.png}
    \caption{Matriz de correlación de los parámetros calculados en la exploración de los datos. }
    \label{fig:figCorrMat}
\end{figure}


\cite{Alvarez2023} definió una matriz de correlación como una tabla que muestra el grado de relación lineal entre un conjunto de variables. En forma de matriz cada fila y columna representa una variable, está a su vez facilita la visualización conjunta de múltiples variables, lo que facilita observar de forma más entendible la exploración de los datos de forma gráfica.


En esta matriz se evidencia que los valores en la diagonal principal siempre serán 1 ya que se encuentran correlacionadas todas las variables, por otro lado tenemos que los valores fuera de la diagonal van de -1 a +1 lo que expresa que mientras más cercano sea el valor a +1 más mayor será la relación positiva, es decir, pueden directamente proporcionales, por otro lado un valor cercano a -1 indica que no existe una relación entre las variables, es decir, pueden ser inversamente proporcionales y un valor cercano a 0 indica que no existe una relación clara entre las variables. \cite{Alvarez2023}


Entre muchas de sus ventajas se resaltan la detección de dependencias o redundancia de variables, lo cual es muy importante y prioritario en campos como análisis de datos y aprendizaje automático y por otro lado nos encontramos que por medio de la implementación de esta técnica se puede simplificar modelos eliminando variables muy correlacionadas que produzcan redundancia en la información. \cite{Alvarez2023}



\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{media/dataset/corr-mat-target.png}
    \caption{Correlación entre la variable objetivo y el resto de parámetros.}
    \label{fig:figCorrMatTarget}
\end{figure}



Muchas variables tienen relaciones muy fuertes (positivas o negativas) entre sí. Por ejemplo,  maximum y minimum tiene una correlación muy cercana a -1. Debido a que las señales en el dataset corresponden a vibraciones, se espera que si se tiene una vibración con una alta amplitud en uno de los ejes positivos, también lo haga en el eje negativo. En el caso de las variables mean a las que se les realizó una transformación logarítmica se obtienen correlaciones altas entre las variables y su respectiva correlación. 


Finalmente se analiza la relación de correlación entre cada parámetro y la categoría si pertenece o no a algún archivo de fallas. La Figura~\ref{fig:figCorrMatTarget} muestra la correlación entre cada parámetro y esa variable objetivo. No hay correlaciones fuertes (mayores a 0.3) entre ninguna variable y anomaly. 
Esto significa que ninguna variable por sí sola explica bien la aparición de anomalías. 
Las variables con mayor asociación, aunque débil, son: \lstinline|log_maximum| (+0.145) y \lstinline|log_std| (+0.112), \lstinline|log_minimum| (-0.152) y \lstinline|length| (-0.216).
Variables como \lstinline|mean| y \lstinline|rpm| no aportan relación clara con la anomalía.



\section{Preparación del dataset de entrenamiento}

En esta etapa se estandarizan el conjunto de datos original con fin que los modelos sean capaces de utilizar los datos para su entrenamiento y predicción. La salida de esta etapa son el conjunto de características y etiquetas que utilizarán los modelos de aprendizaje supervisado para su entrenamiento y evaluación. Los pasos para la construcción del dataset de entrenamiento se visualizan en la Figura~\ref{fig:figGenerationDataset}.


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{media/generacion-dataset.drawio.png}
    \caption{Etapas para la construcción del dataset de entrenamiento.}
    \label{fig:figGenerationDataset}
\end{figure}


En primer lugar, cada archivo del conjunto de datos contiene mediciones de diferentes tipos de acelerómetros. Debido a que los archivos de series de tiempo de condiciones normales, no contienen datos del acelerómetros BA. Por esa razón, se ignoran los datos provenientes de los acelerómetros de tipo BA, puesto que solo están presente en los archivos con datos anómalos. Incluir estos datos genera ruido en el modelo, y en caso que se realice una medición en condiciones normales usando el acelerómetro de tipo BA, el modelo no sería capaz de clasificarlo correctamente.


Adicionalmente, las series de tiempo de los diferentes archivos no tienen la misma longitud, y la cantidad de muestras de los archivos es muy alta. La serie más corta tiene más de 60.000 muestras. Con el fin de generar un conjunto de datos con un misma longitud y una cantidad mayor de registros, se ha decido tomar segmentos de 2048 muestras de cada serie.  De esta manera, de un mismo archivo se pueden obtener muchos segmentos, que se convertirán en una observación para el entrenamiento o evaluación del modelo. 


Los segmentos son seleccionados de forma aleatoria, debido a que hay muchos más archivos que tienen datos de condiciones anómalas en comparación a los archivos de datos en condiciones normales, de éstos últimos se obtienen más observaciones por archivos que de los anteriores. En la salida se obtienen 8192 series de condiciones anómalas de 2048 muestras cada una y la misma cantidad de series para condiciones normales. 


No es recomendable entrenar directamente un modelo con esos valores crudos de los segmentos sin calcular características. En primer lugar, esta alta dimensionalidad de los datos hace a los modelos muy grandes, y si la relación entre el número de observaciones a entrenar y el número de dimensiones de entrada es muy bajo, son más probable que ocurran  efectos negativos como sobreajuste y dificultad para generalizar. 


Por otra parte las series de tiempo suelen tener datos ruidosos y con gran variabilidad local. Esto hace que el modelo se ajuste a patrones irrelevantes.  Además, ruido, un valor pico o un desplazamiento leve temporal puede tener grandes efectos en los resultados del modelo. Extraer features de la serie de tiempo reduce la dimensionalidad, mejora la robustez del modelo y facilita la interpretación de resultados. 


Por lo anterior, una vez obtenidos los segmentos, se procede a la generación de las características de cada conjunto de muestras. Para esto, se calculan una serie de métricas estadísticas descriptivas para cada fila (serie temporal) de la matriz de datos. Estas métricas incluyen:


Maximum: Valor máximo de la serie temporal.
Mean: Media de la serie temporal.
Std: Desviación estándar de la serie temporal.
RMS: Raíz cuadrada del valor medio cuadrático de la serie temporal.
Skewness: Asimetría de la serie temporal.
Kurtosis: Curtosis de la serie temporal.
Crest Factor: Relación entre el valor máximo y el RMS.
Form Factor: Relación entre el RMS y el valor medio absoluto.


Además, se calculan dos métricas adicionales basadas en el espectro en frecuencia de la señal:


THD: Distorsión armónica total.
f0: Frecuencia fundamental..


Finalmente, se agrega información adicional a cada fila del DataFrame, como el RPM, el tipo de anomalía, el diámetro de la falla, el valor de muestreo y la etiqueta de muestreo, así como el nombre del acelerómetro. El resultado final es un DataFrame de Pandas que contiene todas las características extraídas de los segmentos de datos, listo para ser utilizado en tareas de aprendizaje automático.




\chapter{Discusión y análisis de resultados}

Los modelos entrenados tienen muy buenas métricas de rendimiento. 
Los resultados obtenidos en la evaluación de los diferentes modelos de aprendizaje 
automático aplicados a la detección de anomalías muestran un desempeño sobresaliente 
en todas las métricas. 
Al revisar los valores de la Tabla 4, se observa que variables como la exactitud, 
la precisión, la sensibilidad, el F1-score y el área bajo la curva ROC alcanzan 
valores muy altos, superiores en la mayoría de los casos al 0.99. Esto refleja que los 
errores cometidos por los modelos son mínimos, los modelos logran una detección 
confiable de las anomalías y, por tanto, se cumplen los objetivos planteados en 
este proyecto.


El proceso de generar características a partir de los segmentos de serie de tiempo, 
en lugar de utilizar directamente los datos crudos, contribuyó de manera significativa 
a alcanzar estas métricas tan altas. Al aplicar las transformaciones a los datos y 
obtener indicadores estadísticos y espectrales, se logró capturar información relevante 
que permite a los modelos diferenciar con claridad entre estados normales y anómalos, 
mejorando sustancialmente la capacidad predictiva. Entre los parámetros que más 
contribuyeron a la inferencia de la clasificación están la media, la desviación estándar, 
el RMS, la frecuencia fundamental y la distorsión armónica total.


Con respecto al desempeño específico de los modelos, aquellos basados en técnicas de 
gradient boosting, particularmente XGBoost y LightGBM, alcanzaron las métricas más altas, 
mostrando un equilibrio casi perfecto entre precisión y sensibilidad y obtuvieron las 
métricas más altas entre todos los modelos seleccionados. Sin embargo, incluso modelos 
más simples, como la regresión logística, Naive Bayes o KNN, lograron resultados 
sobresalientes, con métricas por encima del 95\%, lo que indica que los datos están muy 
bien representados y que diferentes enfoques de modelado pueden ser útiles en este 
contexto.


Estos resultados demuestran además de ajustarse a los datos, los modelos permiten 
realizar interpretaciones valiosas sobre los factores más influyentes en la detección 
de anomalías. 
Al realizar un análisis detallado de los modelos de regresión logística, árbol de 
decisión y Naive Bayes, se identifican las variables para los modelos a la hora de 
realizar las predicciones. Esas variables fueron la desviación estándar, el valor 
máximo (pico), el RMS, la frecuencia fundamental y la distorsión armónica total THD. 
Esto le da a algunos modelos una doble ventaja: por un lado, una alta capacidad 
predictiva, y por otro, un mayor entendimiento del comportamiento del sistema.


Finalmente, se hace énfasis en la escasa diferencia entre las métricas de los distintos
modelos, resulta difícil declarar un ganador absoluto únicamente con base en su 
rendimiento. 
Para seleccionar el modelo más adecuado para una implementación práctica, deben 
considerarse otros factores además del rendimiento, como la necesidad de ejecutar el 
sistema en tiempo real, la disponibilidad de recursos computacionales y la 
interpretabilidad del modelo. 


En este sentido,  si se busca un modelo con alta interpretabilidad, se aplica el 
principio de parsimonia para seleccionar modelos como Regresión Logística, Naive Bayes,
o Árboles de Decisión, ya que con éstos es  más fácil de explicar el aporte de cada 
variable al  resultado final.  
Si se requiere rapidez y bajo consumo de cómputo, se aplica el mismo principio, y 
nuevamente modelos como regresión logística o los árboles de decisión suelen ser más 
adecuados porque combinan unas muy buenas métricas con un bajo requerimiento de cómputo.
En escenarios donde se requiera una máxima precisión, modelos más complejos como 
XGBoost o LightGBM se perfilan como la mejor opción.



\chapter{Conclusiones y Trabajo Futuro}

En el presente  trabajo se alcanzaron métricas superiores al 90\% en todos los modelos
evaluados, superando el resultado esperado, lo cual refleja la solidez del enfoque 
planteado para la detección de anomalías en entornos industriales. La etapa de 
preprocesamiento de los datos fue un paso clave: la caracterización de las series de 
tiempo, junto con un adecuado balanceo del dataset y la búsqueda de hiperparámetros, 
permitió que los modelos pudieran generalizar mejor y mostraran robustez frente al 
sobreajuste. Estos resultados confirman la efectividad del pipeline desarrollado para 
transformar datos crudos en características discriminativas útiles para los modelos 
de predicción.


Debido al alto rendimiento alcanzado, todos los modelos evaluados podrían aplicarse en 
una solución in-situ. No obstante, los que obtuvieron las mejores métricas fueron 
XGBoost y LightGBM, confirmando su eficacia como algoritmos de referencia en problemas
de clasificación complejos, y un buen referente para la detección de anomalías. Sin 
embargo, si la implementación requiere simplicidad, interpretabilidad y bajo costo 
computacional, los modelos clásicos como la regresión logística, los árboles de decisión
o Naive Bayes resultan altamente recomendables, ya que ofrecen un equilibrio entre 
eficiencia, precisión e interpretabilidad.


Finalmente, gracias al uso de modelos con mayor interpretabilidad, se identificó que 
variables como los valores pico (maximum), la desviación estándar ($std$), el valor RMS ($rms$),
la frecuencia fundamental ($f0$) y la distorsión armónica total ($tdh$) son las que más 
aportan a la clasificación de una observación como anomalía. Este hallazgo  además de 
validar la calidad del modelo, proporciona información valiosa para la toma de 
decisiones en el ámbito industrial, al permitir inferir cuáles son los factores físicos
más determinantes en el diagnóstico temprano de fallos.


Como línea de trabajo futuro, una posibilidad interesante es la integración de los 
modelos desarrollados en un PLC (Controlador Lógico Programable) u otro dispositivo
de cómputo industrial. Esto permitiría que la detección de anomalías se ejecute de 
manera automática y en tiempo real dentro del propio entorno productivo. La adaptación
de los algoritmos a este tipo de hardware representa un reto, especialmente en lo 
relacionado con la optimización del uso de recursos computacionales, pero a la vez abre
la puerta a aplicaciones industriales de gran impacto.


Otra línea de investigación a considerar es el diseño de un algoritmo de clasificación
que no solo detecte anomalías, sino que también identifique el tipo específico de falla:
un clasificador multiclase.  Este enfoque aportaría un valor agregado, ya que permitiría
pasar de un sistema de simple diagnóstico (anómalo/no anómalo) a un sistema de prognosis
y clasificación de fallas, capaz de diferenciar entre problemas la gravedad del fallo y 
el lugar donde se presenta. Con ello, las soluciones podrían evolucionar hacia un 
mantenimiento predictivo más preciso y especializado.


En la Ecuación \eqref{eq:eq1secCTF}


\begin{equation}\label{eq:eq1secCTF}
M=\begin{pmatrix}
	m_{11}&m_{12}\\
	m_{21}&m_{22}
\end{pmatrix}
\end{equation}

En la siguiente Tabla \ref{tab:tab1secCTF}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
	\hline
	1 & 2 \\
	\hline
	22 & 11 \\
	\hline
\end{tabular}
\caption{Tabla 1}
\label{tab:tab1secCTF}
\end{table}

En la siguiente Figura \ref{fig:fig1secCTF}

\begin{figure}[h]
\includegraphics[width= 0.8\textwidth]{logo_unir}
\caption{Logo Unir}
\label{fig:fig1secCTF}
\end{figure}

% \cite{PIMENTEL2016744} \cite{da_S_Bessa_2023}

%\begin{thebibliography}{a}
%\bibitem{etiqueta} \textsc{Autores},
%\textit{nombre referencia.}
%Información addicional
%\end{thebibliography}
\bibliographystyle{apacite}
\bibliography{bibliografia}

\appendix
\chapter{Apendices}

\end{document}





















